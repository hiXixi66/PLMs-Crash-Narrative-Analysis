
# PLMS-CRASH-NARRATIVE-ANALYSIS

This project contains data processing, model fine-tuning, and evaluation scripts for **traffic crash narrative analysis**.  
It supports **large language models (PLMs)** (e.g., Qwen, LLaMA, BERT) and classic NLP models (TextRNN, FastText) for tasks such as:
- **MANCOLL classification** (manner of collision)
- **Crash Category (CCat)**
- **Crash Configuration (CConf)**
- **Crash Type (CTp)**

The project is expected to run on **GPU-enabled servers**.

---

## ğŸ“‚ Project Structure

```

PLMS-CRASH-NARRATIVE-ANALYSIS/
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ consis-compute.py          # Compute cross-model consistency (pairwise agreement, Fleiss' kappa)
â”‚   â”œâ”€â”€ mancoll-unknown-class.py   # Analyze the outputs of PLMs on unknown MANCOLL classes
â”‚   â”œâ”€â”€ self-cons.py               # Self-consistency analysis for multiple runs
â”‚   â””â”€â”€ US-DB.txt                  # U.S. database reference
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed_data/            # Cleaned & processed Excel data
â”‚   â””â”€â”€ raw-data/                  # Original raw crash data
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_process/
â”‚   â”‚   â”œâ”€â”€ extract_excel_info.py  # Extract and clean data from the massive datset
â”‚   â”‚   â””â”€â”€ NoiseTest_data_gen.py  # Generate noisy datasets for robustness testing
â”‚   â”‚
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ bert-noft.py                   # BERT feature extraction (no fine-tuning)
â”‚       â”œâ”€â”€ finetune_bert_crashtype.py     # Fine-tune BERT for Crash Type
â”‚       â”œâ”€â”€ finetune_bert_mancoll.py       # Fine-tune BERT for MANCOLL classification
â”‚       â”œâ”€â”€ finetune_bert_mancoll_noise.py # Fine-tune BERT with noisy MANCOLL labels
â”‚       â”œâ”€â”€ llm_loader_HPC.py              # **Central config**: specify model storage paths (Qwen, LLaMA, Mistral, BERT)
â”‚       â”œâ”€â”€ mancoll-fastText-finetune.py   # FastText baseline
â”‚       â”œâ”€â”€ mancoll-rnn-finetune.py        # RNN baseline
â”‚       â”œâ”€â”€ model_finetune_CCat.py         # Fine-tune LLM for Crash Category
â”‚       â”œâ”€â”€ model_finetune_Cconf.py        # Fine-tune LLM for Crash Configuration
â”‚       â”œâ”€â”€ model_finetune_CTp.py          # Fine-tune LLM for Crash Type
â”‚       â”œâ”€â”€ model_finetune-withNoise.py    # Fine-tune LLM with noisy labels
â”‚       â””â”€â”€ model_finetune.py              # Fine-tune LLM for manner of collision
â”‚
â””â”€â”€ tests/
â”œâ”€â”€ CrashCAT_test.py
â”œâ”€â”€ CrashCONF_test.py
â”œâ”€â”€ CrashType_test.py
â”œâ”€â”€ CrashType_test-analysis.py
â””â”€â”€ MANCOLL_test.py

````

---

## âš™ï¸ Model Storage Configuration

Edit `src/llm/llm_loader_HPC.py` to set the storage paths of your LLMs (on HPC or local server):

All training scripts import this file to load models automatically.

---

## ğŸš€ Running on GPU Cluster

Each main training/evaluation script has a corresponding `.sh` launcher (for SLURM or similar HPC schedulers).

Example SLURM template:

```bash
#!/bin/bash
#SBATCH --job-name=finetune_mancoll
#SBATCH --gres=gpu:1          # Number of GPUs
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --time=24:00:00
#SBATCH --output=logs/mancoll_%j.out

module load cuda/11.8
source ~/envs/plms/bin/activate   # activate your Python virtual env

python src/llm/finetune_bert_mancoll.py
```

Recommended `.sh` scripts:

| Script name                   | Purpose                                                 |
| ----------------------------- | ------------------------------------------------------- |
| `train_mancoll_bert.sh`       | Fine-tune BERT for MANCOLL                              |
| `train_mancoll_bert_noise.sh` | Fine-tune BERT with noisy MANCOLL labels                |
| `train_crashcat_llm.sh`       | Fine-tune LLM for Crash Category                        |
| `train_crashconf_llm.sh`      | Fine-tune LLM for Crash Configuration                   |
| `train_crashtype_llm.sh`      | Fine-tune LLM for Crash Type                            |
| `train_llm_with_noise.sh`     | Fine-tune LLM with noisy labels                         |
| `fasttext_baseline.sh`        | Train FastText model                                    |
| `rnn_baseline.sh`             | Train RNN model                                         |
| `consistency_analysis.sh`     | Run self-consistency and cross-model agreement analysis |

---

### Example: `train_mancoll_bert.sh`

```bash
#!/bin/bash
#SBATCH --job-name=mancoll_bert
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --time=12:00:00
#SBATCH --output=logs/mancoll_bert_%j.out

source ~/envs/plms/bin/activate
python src/llm/finetune_bert_mancoll.py
```


---

## ğŸ› ï¸ Installation

```bash
conda create -n plms python=3.10
conda activate plms

pip install -r requirements.txt
```

`requirements.txt` includes:
`torch`, `transformers`, `datasets`, `peft`, `accelerate`, `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, `statsmodels`, `openpyxl`.

---

## ğŸ“Š Outputs

* **Model checkpoints** â†’ `models/`
* **Analysis reports** â†’ `reports/`
* **Plots & heatmaps** â†’ `reports/.../plots/`
* **Consistency metrics** â†’ CSV files (`consistency_matrix.csv`, `sample_consistency.csv`)

